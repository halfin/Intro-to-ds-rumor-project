{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import re\n",
    "from snownlp import SnowNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##parameter\n",
    "data_place = r\"E:\\网络学堂\\数据科学导论\\final project\\Chinese_Rumor_Dataset-master\\Chinese_Rumor_Dataset-master\\CED_Dataset\\original-microblog\"\n",
    "\n",
    "##hyper-parameter\n",
    "top_word = 20##分词超参数\n",
    "allowpos = ('ns', 'n', 'vn', 'v','a','e','d','o','r')##关键词属性设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##feature\n",
    "##content feature\n",
    "corpus = []##语料库\n",
    "excalmatory_mark = []##感叹号个数\n",
    "question_mark = []##问号个数\n",
    "postive_words = []##褒义词个数\n",
    "negative_words = []##贬义词个数\n",
    "positive_rate = []##这条微博的积极性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##weibo feature\n",
    "has_url = []##微博当中是否含有超链接\n",
    "##user feature\n",
    "description = []##原始微博发布者是否有描述\n",
    "gender = []##原始微博发布者性别\n",
    "followers = []##原始微博发布者被关注量\n",
    "friends = []##原始微博发布者朋友数\n",
    "vertified = []##用户是否认证\n",
    "##label\n",
    "category = []##类别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##共3300个数据，1451个谣言\n",
    "def read_data(path):\n",
    "    global corpus\n",
    "    global gender\n",
    "    global followers\n",
    "    global friends\n",
    "    global vertified\n",
    "    global description\n",
    "    global has_url\n",
    "    global category\n",
    "    path_list = os.listdir(path)\n",
    "    for filename in path_list:\n",
    "        if os.path.splitext(filename)[1] == '.json':\n",
    "            with open(path + \"\\\\\" + filename, encoding='utf-8') as fp:\n",
    "                data = json.load(fp)\n",
    "                if(data['user'] == 'empty'):\n",
    "                    continue\n",
    "                corpus.append(re.sub(r'[^\\u4E00-\\u9FA5?？!！]',\"\",data['text']))\n",
    "                description.append(data['user']['description'])\n",
    "                gender.append(data['user']['gender'])\n",
    "                followers.append(data['user']['followers'])\n",
    "                friends.append(data['user']['friends'])\n",
    "                has_url.append(data['has_url'])\n",
    "                vertified.append(data['user']['verified'])\n",
    "                if(int(re.match(\"\\d+\",filename).group()) <= 2600):\n",
    "                    category.append(1)\n",
    "                if(int(re.match(\"\\d+\",filename).group()) > 2600):\n",
    "                    category.append(0)\n",
    "\n",
    "    has_url = np.array(has_url).astype(int)\n",
    "    vertified = np.array(vertified).astype(int)\n",
    "    description = np.array(description).astype(int)\n",
    "    gender = np.array(list(map(trans_gender, gender)))\n",
    "    followers = np.array(followers)\n",
    "    friends = np.array(friends)\n",
    "    category = np.array(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_gender(gender):\n",
    "    if(gender == \"m\"):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "##统计感叹号与问号的个数\n",
    "def count_punction():\n",
    "    global excalmatory_mark\n",
    "    global question_mark\n",
    "    excalmatory_mark = np.array([i.count('！') + i.count('!') for i in corpus])\n",
    "    question_mark = np.array([i.count('？') + i.count('?') for i in corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##acquire text features\n",
    "##处理原始微博的褒贬属性(统计褒贬词的个数[根据词的情感态度]，分析一条微博的积极性，算法为——\n",
    "##将一条微博的前top_word个关键词找出，其排序的方式为TF-IDF，TF-IDF对应的总语料库为jieba分词总语料库\n",
    "##将这些词(情感系数-0.5)关于其重要性加权平均得到这条微博的积极性)\n",
    "def handle_corpus():\n",
    "    global positive_rate\n",
    "    global postive_words\n",
    "    global negative_words\n",
    "    global corpus\n",
    "    for sent in corpus:\n",
    "        postive_num = 0\n",
    "        negative_num = 0\n",
    "        rate = 0\n",
    "        for x, w in jieba.analyse.textrank(sent, topK=top_word, withWeight=True,allowPOS=allowpos):\n",
    "            rate += (SnowNLP(x).sentiments - 0.5) * w\n",
    "            if (SnowNLP(x).sentiments < 0.4):\n",
    "                negative_num += 1\n",
    "            if (SnowNLP(x).sentiments > 0.6):\n",
    "                postive_num += 1\n",
    "        ##加上最后一个0.000000001是因为有的微博没有关键词，所以为了防止分母为0加上一个微小数\n",
    "        rate = rate/(len(jieba.analyse.textrank(sent, topK=top_word, withWeight=True, allowPOS=allowpos))+0.000000001)\n",
    "        postive_words.append(postive_num)\n",
    "        negative_words.append(negative_num)\n",
    "        positive_rate.append(rate)\n",
    "    postive_words = np.array(postive_words)\n",
    "    negative_words = np.array(negative_words)\n",
    "    positive_rate = np.array(positive_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\maohn\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.705 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "##read json document and acquire corpus\n",
    "df = read_data(data_place)\n",
    "## Given a corpus:\n",
    "count_punction()\n",
    "handle_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " For friends, max is: 5000  min is: 0  mean is: 920.7363636363636 median is: 626.0\n",
      " For ! , max is: 17  min is: 0  mean is: 0.9263636363636364 median is: 0.0\n",
      " For ? , max is: 9  min is: 0  mean is: 0.2787878787878788 median is: 0.0\n",
      " For positive rate, max is: 0.368421052263158  min is: -0.2859330339355865  mean is: 0.03880970856454348 median is: 0.034656072931484226\n",
      " For positive words, max is: 16  min is: 0  mean is: 5.553333333333334 median is: 6.0\n",
      " For negative words, max is: 10  min is: 0  mean is: 2.6054545454545455 median is: 2.0\n"
     ]
    }
   ],
   "source": [
    "##Take a look at the data:\n",
    "print(' For friends, max is:',friends.max(),' min is:',friends.min(),' mean is:',friends.mean(),'median is:',np.median(friends))\n",
    "print(' For ! , max is:',excalmatory_mark.max(),' min is:',excalmatory_mark.min(),' mean is:',excalmatory_mark.mean(),'median is:',np.median(excalmatory_mark))\n",
    "print(' For ? , max is:',question_mark.max(),' min is:',question_mark.min(),' mean is:',question_mark.mean(),'median is:',np.median(question_mark))\n",
    "print(' For positive rate, max is:',positive_rate.max(),' min is:',positive_rate.min(),' mean is:',positive_rate.mean(),'median is:',np.median(positive_rate))\n",
    "print(' For positive words, max is:',postive_words.max(),' min is:',postive_words.min(),' mean is:',postive_words.mean(),'median is:',np.median(postive_words))\n",
    "print(' For negative words, max is:',negative_words.max(),' min is:',negative_words.min(),' mean is:',negative_words.mean(),'median is:',np.median(negative_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " For has_url, number of 1 is: 635 number of 0 is: 2665\n",
      " For description, number of 1 is: 3139 number of 0 is: 161\n",
      " For gender, number of 1 is: 2053 number of 0 is: 1247\n",
      " For vertified, number of 1 is: 1926 number of 0 is: 1374\n",
      " For labels, number of 1 is: 1451 number of 0 is: 1849\n"
     ]
    }
   ],
   "source": [
    "print(' For has_url, number of 1 is:',np.count_nonzero(has_url),'number of 0 is:',len(has_url)-np.count_nonzero(has_url))\n",
    "print(' For description, number of 1 is:',np.count_nonzero(description),'number of 0 is:',len(description)-np.count_nonzero(description))\n",
    "print(' For gender, number of 1 is:',np.count_nonzero(gender),'number of 0 is:',len(gender)-np.count_nonzero(gender))\n",
    "print(' For vertified, number of 1 is:',np.count_nonzero(vertified),'number of 0 is:',len(vertified)-np.count_nonzero(vertified))\n",
    "print(' For labels, number of 1 is:',np.count_nonzero(category),'number of 0 is:',len(category)-np.count_nonzero(category))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##regulation:\n",
    "rexcalmatory = excalmatory_mark/(excalmatory_mark.max()-excalmatory_mark.min())\n",
    "rquestion = question_mark/(question_mark.max()-question_mark.min())\n",
    "rfriend = friends/(friends.max()-friends.min())\n",
    "rfollowers = followers/(followers.max()-followers.min())\n",
    "rpositive_words = postive_words/(postive_words.max()-postive_words.min())\n",
    "rnegative_words = negative_words/(negative_words.max()-negative_words.min())\n",
    "rpositive_rate = positive_rate/(positive_rate.max()-positive_rate.min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##X2 = np.array(has_url,description,gender,vertified)\n",
    "X1 = []\n",
    "X1.append(rexcalmatory.tolist())\n",
    "X1.append(rquestion.tolist())\n",
    "X1.append(rfriend.tolist())\n",
    "X1.append(rfollowers.tolist())\n",
    "X1.append(rpositive_words.tolist())\n",
    "X1.append(rnegative_words.tolist())\n",
    "X1.append(rpositive_rate.tolist())\n",
    "X1.append(has_url.tolist())\n",
    "X1.append(description.tolist())\n",
    "X1.append(gender.tolist())\n",
    "X1.append(vertified.tolist())\n",
    "Y = category\n",
    "X1 = np.array(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.05882353, 0.        , 0.1814    , ..., 1.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.05882353, 0.11111111, 0.3846    , ..., 0.        , 1.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.11111111, 0.0164    , ..., 1.        , 1.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.064     , ..., 1.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.05882353, 0.        , 0.496     , ..., 1.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.05882353, 0.        , 0.3124    , ..., 1.        , 1.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conX=X1.T\n",
    "conX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(conX, Y, test_size=0.3, random_state=88, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11764706, 0.22222222, 0.4898    , ..., 1.        , 1.        ,\n",
       "        1.        ],\n",
       "       [0.47058824, 0.        , 0.4038    , ..., 1.        , 1.        ,\n",
       "        1.        ],\n",
       "       [0.05882353, 0.11111111, 0.3874    , ..., 1.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.3954    , ..., 0.        , 1.        ,\n",
       "        1.        ],\n",
       "       [0.        , 0.        , 0.131     , ..., 1.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.05882353, 0.        , 0.2464    , ..., 1.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the KNN is 0.7131313131313132\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=50)\n",
    "\n",
    "# change the shape of Y_train to (n_samples, ) using `.ravel()`\n",
    "knn.fit(X_train, Y_train.ravel())\n",
    "\n",
    "knn_pred = knn.predict(X_test)\n",
    "\n",
    "print('The accuracy of the KNN is', metrics.accuracy_score(knn_pred, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Logistic Regression is 0.7686868686868686\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(penalty='l1',max_iter=800,fit_intercept=False,solver='saga',C=20)\n",
    "clf.fit(X_train, Y_train.ravel())\n",
    "clf_pred = clf.predict(X_test)\n",
    "print('The accuracy of the Logistic Regression is', metrics.accuracy_score(clf_pred, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Decision Tree is 0.6787878787878788\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "dtree = tree.DecisionTreeClassifier()\n",
    "dtree = dtree.fit(X_train, Y_train.ravel())\n",
    "dtree_pred = dtree.predict(X_test)\n",
    "print('The accuracy of the Decision Tree is', metrics.accuracy_score(dtree_pred, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Random Forest is 0.7858585858585858\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(\n",
    "    n_estimators = 100,\n",
    "    criterion = 'gini',\n",
    "    random_state = 0\n",
    ")\n",
    "rfc.fit(X_train, Y_train.ravel())\n",
    "rfc_pred = rfc.predict(X_test)\n",
    "print('The accuracy of the Random Forest is', metrics.accuracy_score(rfc_pred, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Random Forest is 0.7717171717171717\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(\n",
    "    n_estimators = 100,\n",
    "    criterion ='entropy',\n",
    "    random_state = 0\n",
    ")\n",
    "rfc.fit(X_train, Y_train.ravel())\n",
    "rfc_pred = rfc.predict(X_test)\n",
    "print('The accuracy of the Random Forest is', metrics.accuracy_score(rfc_pred, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the AdaBoost is 0.7696969696969697\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "abc = AdaBoostClassifier()\n",
    "abc.fit(X_train, Y_train.ravel())\n",
    "abc_pred = abc.predict(X_test)\n",
    "print('The accuracy of the AdaBoost is', metrics.accuracy_score(abc_pred, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，在学习模型中，表现最好的是RandomForest模型，下面进一步调参："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 50, 'max_leaf_nodes': 82, 'n_estimators': 70}\n",
      "0.7913419913419913\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'n_estimators': np.arange(10, 120,10),'max_depth':np.arange(10,100,10),'max_leaf_nodes':np.arange(2,100,20)}\n",
    "\n",
    "Rfc = RandomForestClassifier()\n",
    "Rfc_cv = GridSearchCV(Rfc, param_grid, cv=3)\n",
    "\n",
    "# change the shape of Y_train to (n_samples, ) using `.ravel()`\n",
    "Rfc_cv.fit(X_train, Y_train.ravel())\n",
    "\n",
    "print(Rfc_cv.best_params_)\n",
    "print(Rfc_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 50, 'max_leaf_nodes': 100, 'n_estimators': 65}\n",
      "0.7917748917748918\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'n_estimators': np.arange(60, 80, 5),'max_depth':np.arange(40,60,5),'max_leaf_nodes':np.arange(60,120,10)}\n",
    "\n",
    "Rfc = RandomForestClassifier()\n",
    "Rfc_cv = GridSearchCV(Rfc, param_grid, cv=3)\n",
    "\n",
    "# change the shape of Y_train to (n_samples, ) using `.ravel()`\n",
    "Rfc_cv.fit(X_train, Y_train.ravel())\n",
    "\n",
    "print(Rfc_cv.best_params_)\n",
    "print(Rfc_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 47, 'max_leaf_nodes': 99, 'n_estimators': 63}\n",
      "0.793073593073593\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'n_estimators': np.arange(62, 68, 1),'max_depth':np.arange(46,52,1),'max_leaf_nodes':np.arange(90,100,1)}\n",
    "\n",
    "Rfc = RandomForestClassifier()\n",
    "Rfc_cv = GridSearchCV(Rfc, param_grid, cv=3)\n",
    "\n",
    "# change the shape of Y_train to (n_samples, ) using `.ravel()`\n",
    "Rfc_cv.fit(X_train, Y_train.ravel())\n",
    "\n",
    "print(Rfc_cv.best_params_)\n",
    "print(Rfc_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 120}\n",
      "0.7854545454545454\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'n_estimators': np.arange(10, 200, 10)}\n",
    "\n",
    "Rfc = RandomForestClassifier()\n",
    "Rfc_cv = GridSearchCV(Rfc, param_grid, cv=3)\n",
    "\n",
    "# change the shape of Y_train to (n_samples, ) using `.ravel()`\n",
    "Rfc_cv.fit(conX, Y.ravel())\n",
    "\n",
    "print(Rfc_cv.best_params_)\n",
    "print(Rfc_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 118}\n",
      "0.7874458874458874\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'n_estimators': np.arange(110, 130, 2)}\n",
    "\n",
    "Rfc = RandomForestClassifier()\n",
    "Rfc_cv = GridSearchCV(Rfc, param_grid, cv=3)\n",
    "\n",
    "# change the shape of Y_train to (n_samples, ) using `.ravel()`\n",
    "Rfc_cv.fit(X_train, Y_train.ravel())\n",
    "\n",
    "print(Rfc_cv.best_params_)\n",
    "print(Rfc_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Random Forest is 0.7888888888888889\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(\n",
    "    n_estimators = 118,\n",
    "    criterion = 'gini',\n",
    "    random_state = 0\n",
    ")\n",
    "rfc.fit(X_train, Y_train.ravel())\n",
    "rfc_pred = rfc.predict(X_test)\n",
    "print('The accuracy of the Random Forest is', metrics.accuracy_score(rfc_pred, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看各特征的重要性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06132879 0.02345669 0.1518104  0.35426329 0.08046176 0.07246634\n",
      " 0.15069153 0.02662153 0.00826813 0.02351406 0.04711749]\n"
     ]
    }
   ],
   "source": [
    "rfcf=RandomForestClassifier(\n",
    "    n_estimators=118,\n",
    "    criterion = 'gini',\n",
    "    random_state = 0\n",
    ")\n",
    "rfcf.fit(X_train, Y_train.ravel())\n",
    "print(rfcf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中依次对应：包含感叹号次数，包含问号次数，好友数，被关注量，正面词汇，负面词汇，正面率，是否包含链接，是否包含描述，性别，是否验证\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
